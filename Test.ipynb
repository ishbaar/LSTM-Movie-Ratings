{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#source: https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "\n",
    "#This function takes in the path to the glove pretrained vectors that is\n",
    "#originally stored as a .txt and then converts it into a dictionary. To access\n",
    "#a certain vector of a certain word, use the word as the key, and the dictionary\n",
    "#will return the vector.\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',encoding=\"utf-8\")\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "#This function takes in the path to our modified csv file and returns individual\n",
    "#lists containing title, summary, and score, respectively. \n",
    "#\n",
    "#Example: title[k] returns the movie at index k's title in the form of a string\n",
    "#\n",
    "#@ Params: csvFile - the path to our modified csv file containing only the usable\n",
    "#                    movies\n",
    "#\n",
    "#@ Return: title - a list of all the movie titles inside of csvFile\n",
    "#          summary - a list of all the summaries inside of csvFile\n",
    "#          score - a list of all the scores (voter averages) inside of csvFile\n",
    "def parseCSV(csvFile):\n",
    "    #define the column names\n",
    "    \n",
    "    #reading the file to parse\n",
    "    with open(csvFile, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        read_list = list(reader)\n",
    "    \n",
    "    #stores the respective data in lists\n",
    "    title = []\n",
    "    summary = []\n",
    "    score = []\n",
    "    \n",
    "    # len(read_list)\n",
    "    for i in range(1, 1000):\n",
    "        title.append(read_list[i][0])\n",
    "        summary.append(read_list[i][1])\n",
    "        score.append(float(read_list[i][2]))\n",
    "    #because the first index of each list is the header, we get rid of\n",
    "    #the first element in each list\n",
    "\n",
    "    \n",
    "    return title, summary, score\n",
    "\n",
    "#This function takes in the summary list and then randomly generates a test\n",
    "#set and a training set. The hardcoded value is 80 percent training and 20\n",
    "#percent test. This returns a trainingSet, a testSet, a testSetIndex, and\n",
    "#a trainingSetIndex list. \n",
    "#\n",
    "#@ Params: summary - the summary list generated by the function parseCSV\n",
    "#@ Return: trainingSet - the summaries inside of the parameter summary to be used\n",
    "#                        as training data\n",
    "#          testSet - the summaries inside of the parameter summary to be used for\n",
    "#                    testing purposes\n",
    "#          testSetIndex - a list of integers that represent which indices inside\n",
    "#                         summary are being used for testing purposes\n",
    "#          trainingSetIndex - a list of integers that represent which indices inside\n",
    "#                             summary are being used for training purposes.\n",
    "def generateSets(summary):\n",
    "    #definition of constants throughout this function\n",
    "    training_percentage = .8\n",
    "    list_size = len(summary) - 1\n",
    "    \n",
    "    #initialize the things to return\n",
    "    testSetIndex = []\n",
    "    testSet = []\n",
    "    trainingSet = []\n",
    "    trainingSetIndex = []\n",
    "    \n",
    "    #Steps to generate the test set:\n",
    "    #1. Randomly generate an integer between 0 and list_size\n",
    "    #2. Check to see if this index is already within our test set\n",
    "    #3. If it isn't, document this index inside our testSetIndex and go on\n",
    "    #4. If it is, generate another random number until we find one that isn't\n",
    "    #5. Repeat until testSetIndex is of size (1-training_percentage)\n",
    "    for i in range(int(list_size * (1-training_percentage))):\n",
    "        rand_index = random.randint(0, list_size)\n",
    "        \n",
    "        #keep generating until it is NOT inside of testSetIndex\n",
    "        while(rand_index in testSetIndex):\n",
    "            rand_index = random.randint(0, list_size)\n",
    "            \n",
    "        testSetIndex.append(rand_index)\n",
    "        testSet.append(summary[rand_index])\n",
    "        \n",
    "    #After this, we will have int[(1-training_percentage)*list_size] elements\n",
    "    #inside of testSet. Training set is everything else. \n",
    "    for k in range(list_size):\n",
    "        \n",
    "        if(k not in testSetIndex):\n",
    "            trainingSet.append(summary[k])\n",
    "            trainingSetIndex.append(k)\n",
    "            \n",
    "    return trainingSet, testSet, testSetIndex, trainingSetIndex\n",
    "\n",
    "#taken and modified from: \n",
    "#https://stackoverflow.com/questions/9797357/dividing-a-string-at-various-punctuation-marks-using-split\n",
    "\n",
    "#This function splits the sentence by spaces and punctuation marks. Punctuation\n",
    "#marks refer to non-alphanumeric characters that can appear validly inside\n",
    "#of the English sentence. One exception to this rule is the string \"'s\" as this\n",
    "#has it's own vector inside of the glove pretrained vector set. Therefore, this\n",
    "#particular string will get it's own index. \n",
    "#\n",
    "#@Param: text- the text to be parsed. This must a valid English block of text,\n",
    "#              as in, it must be readable to the everyday person\n",
    "#@Retuen: sentence - a list containing the parsed version of the input text.\n",
    "#                    Each index of this will contain either a punctuation mark\n",
    "#                    by itself, the string \"'s\" by itself, or an English word.\n",
    "def parseSentence(text):\n",
    "    sentence = (\"\".join((char if char.isalnum() else (\" \"+ char + \" \")) for char in text).split())\n",
    "    \n",
    "    #join the instances of \"'s\" and ONLY \"'s\"\n",
    "    i = 0\n",
    "    while i in range (len(sentence)):\n",
    "        if(sentence[i] == \"'\" and i+1 in range(len(sentence)) and sentence[i+1] == 's'):\n",
    "            sentence[i] = \"'s\"\n",
    "            sentence.pop(i+1)\n",
    "        i += 1\n",
    "            \n",
    "    #join the rest of the punctuation marks. AKA non-alphanumeric characters\n",
    "    j = 0\n",
    "    while j in range (len(sentence)): \n",
    "        if(sentence[j] != \"'s\" and not sentence[j].isalnum()):\n",
    "            k = j+1\n",
    "            while (k in range (len(sentence)) and not sentence[k].isalnum()):\n",
    "                sentence[j] = sentence[j] + sentence[k]\n",
    "                sentence.pop(k)\n",
    "        \n",
    "        j += 1\n",
    "    \n",
    "    #Because the glove database is all lowercase, we need to lowercase everything\n",
    "    for k in range(len(sentence)):\n",
    "        sentence[k] = sentence[k].lower()\n",
    "    \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "#Definition of our MLP model. The input layer has input size of glove_dim which\n",
    "#is the dimensionality of the GloVe dataset we are currently using. This has 5\n",
    "#hidden layers, all with 100 nodes and a output layer of 1, which is our predicted\n",
    "#user rating(voter average)\n",
    "#\n",
    "#@Param: glove_dim - the dimensionality of the GloVe dataset we are currently using\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, glove_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            #Input layer: dimension of GloVe embedding we are currently using\n",
    "            nn.Linear(glove_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            #output layer: 1\n",
    "            nn.Linear(100, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "#This function defines a LSTM and a MLP and trains the MLP in order to generate\n",
    "#predicted ratings for a given summary. It takes in the dictionary that contains\n",
    "#all the GloVe pre-trained word embeddings. It first calls parseCSV to parse the\n",
    "#CSV file containing all the movies. Then it calls generateSets to define our\n",
    "#test and training sets. For each summary inside of our training set, it feeds\n",
    "#the summary into the LSTM and then uses the final hidden state as the vector\n",
    "#representation for the entire summary. The MLP takes in this vector and generates\n",
    "#a predicted rating. We perform SGD on the loss function in order to train the MLP.\n",
    "#After training, for every test set, the MLP predicts a rating and then the\n",
    "#difference is appended to a list and the list is returned after going through\n",
    "#all the test sets.\n",
    "#\n",
    "#@Param: vec_dict - the dictonary containing the GloVe pre-trained vector representations\n",
    "#                   of words\n",
    "#        vec_dict_dim - the dimensionality of the GloVe dataset you are using.\n",
    "#                       Should be one of the following numbers: 50, 100, 200, 300\n",
    "#@Return: differences - A list containing tuples of:\n",
    "#                       (abs(predicted_value - actual_value),\n",
    "#                       the index of where this difference occured)\n",
    "def train(vec_dict, vec_dict_dim):\n",
    "    #Define our constants\n",
    "    vec_model = vec_dict\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    title, summary, score = parseCSV('tmdb_5000_movies_modified.csv')\n",
    "    trainingSet, testSet, testSetIndex, trainingSetIndex = generateSets(summary)\n",
    "    torch.manual_seed(1)\n",
    "    \n",
    "\n",
    "    glove_dim = vec_dict_dim\n",
    "    \n",
    "    #Define our models \n",
    "    mlp_model = MLP(glove_dim)\n",
    "    optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.001)\n",
    "    lstm = nn.LSTM(glove_dim,glove_dim)\n",
    "   \n",
    "    \n",
    "######################### START TRAINING ######################################\n",
    "    \n",
    "    #For each training set\n",
    "    for i in range(len(trainingSet)):\n",
    "        parsed_summary = parseSentence(trainingSet[i])\n",
    "        associated_index = trainingSetIndex[i]\n",
    "        inputs = []\n",
    "        \n",
    "        #populate the input list with our word embeddings\n",
    "        for j in range(len(parsed_summary)):\n",
    "            #Make sure the string exists inside of the glove model\n",
    "            if(parsed_summary[j] in vec_model):\n",
    "                inputs.append(torch.from_numpy(vec_model[parsed_summary[j]]).float())\n",
    "                \n",
    "        #initialize hidden states\n",
    "        hidden = (torch.ones(1, 1, glove_dim), torch.ones(1, 1, glove_dim))\n",
    "        \n",
    "        #for each index inside of our inputs, we input it into the lstm sequentially\n",
    "        for k in inputs:\n",
    "            out, hidden = lstm(k.view(1, 1,-1), hidden)\n",
    "            \n",
    "        #we do not use the cell state, so we need to call hidden[0] when inputting\n",
    "        #into MLP\n",
    "        predicted_value = mlp_model(hidden[0])\n",
    "        \n",
    "        #use associated_index to find the true value inside of our score list\n",
    "        actual_value = torch.tensor(score[associated_index], requires_grad=True)\n",
    "        \n",
    "        #clears the gradient as Pytorch accumulates gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #find our total loss\n",
    "        loss = loss_fn(predicted_value, actual_value)\n",
    "        \n",
    "        #gradient descent\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i % 50 == 0):\n",
    "            print(\"Progress \", i / len(trainingSet), \"%\", end=\"\\r\")\n",
    "            \n",
    "        \n",
    "######################### END TRAINING #######################################\n",
    "    \n",
    "######################### START TESTING ######################################\n",
    "    \n",
    "    #This list will store the difference between our predicted value vs actual\n",
    "    differences = []\n",
    "    #For each test set\n",
    "    for i in range(len(testSet)):\n",
    "        parsed_summary = parseSentence(testSet[i])\n",
    "        associated_index = testSetIndex[i]\n",
    "        inputs = []\n",
    "        \n",
    "        #populate the input list with our word embeddings\n",
    "        for j in range(len(parsed_summary)):\n",
    "            #Make sure the string exists inside of the glove model\n",
    "            if(parsed_summary[j] in vec_model):\n",
    "                inputs.append(torch.from_numpy(vec_model[parsed_summary[j]]).float())\n",
    "                \n",
    "        #initialize hidden states\n",
    "        hidden = (torch.ones(1, 1, glove_dim), torch.ones(1, 1, glove_dim))\n",
    "        \n",
    "        #for each index inside of our inputs, we input it into the lstm sequentially\n",
    "        for k in inputs:\n",
    "            out, hidden = lstm(k.view(1, 1,-1), hidden)\n",
    "            \n",
    "        #we do not use the cell state, so we need to call hidden[0] when inputting\n",
    "        #into MLP\n",
    "        predicted_value = mlp_model(hidden[0]).item()\n",
    "        actual_value = score[associated_index]\n",
    "\n",
    "        temp_tuple = (abs(predicted_value - actual_value), associated_index)\n",
    "        differences.append(temp_tuple)\n",
    "        \n",
    "######################### END TESTING ########################################\n",
    "        \n",
    "    return differences\n",
    "\n",
    "#This function takes in the differences list from the train() function and it\n",
    "#outputs the accuracy of the testing, the maximum difference, and the index at\n",
    "#where this maximum difference occured.\n",
    "#\n",
    "#@Param: differences - the list returned by the function train(). This should\n",
    "#                      be a list containing tuples of the form:\n",
    "#                      (abs(predicted_value - actual_value), \n",
    "#                       index at where this difference occured)\n",
    "#@Return: final_accuracy - A float that represents the accuracy of our testing\n",
    "#         max_diff - The maximum difference inside of the list difference\n",
    "#         max_diff_index - the index at where max_diff occurs\n",
    "def calculate_accuracy(differences):\n",
    "    #initialize the differences to the first element\n",
    "    max_diff = differences[0][0]\n",
    "    max_diff_index = differences[0][1]\n",
    "\n",
    "    #initialize our constant\n",
    "    size = len(differences)\n",
    "    \n",
    "    #initialize our counter\n",
    "\n",
    "    num_correct_0 = 0\n",
    "    num_correct_25 = 0\n",
    "    num_correct_5 = 0\n",
    "    num_correct_1 = 0\n",
    "    \n",
    "    #definition of what is \"accurate\". Anything <= MARGIN_ERROR is deemed accurate.\n",
    "    MARGIN_ERROR = .25\n",
    "    \n",
    "    #iterate through the list of differences\n",
    "    for element in differences:\n",
    "        \n",
    "        #check for maximum difference\n",
    "        if element[0] > max_diff:\n",
    "            max_diff = element[0]\n",
    "            max_diff_index = element[1]\n",
    "            \n",
    "        #this gets the difference in predicted vs actual\n",
    "        diff = element[0]\n",
    "        \n",
    "        if diff <= .25:\n",
    "            num_correct_25 += 1\n",
    "        if diff == 0:\n",
    "            num_correct_0 += 1\n",
    "        if diff <= 1:\n",
    "            num_correct_1 += 1\n",
    "    \n",
    "    #accuracy = (accurate predictions / size of list)\n",
    "    final_accuracy_0 = num_correct_0/size\n",
    "    final_accuracy_25 = num_correct_25/size\n",
    "    final_accuracy_5 = num_correct_5/size\n",
    "    final_accuracy_1 = num_correct_1/size\n",
    "    \n",
    "    array = np.array(differences)\n",
    "    print(array)\n",
    "    return final_accuracy_0, final_accuracy_25,final_accuracy_5, final_accuracy_1, np.average(array,)  # , max_diff, max_diff_index\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n",
      "Finished loading vect_dict in 14.713267134231355 seconds \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path variables\n",
    "GLOVE_PATH= './glove.6B.50d.txt'\n",
    "MOVIE_CSV = 'tmdb_5000_movies_modified.csv'\n",
    "MARGIN_ERROR = 1 # How large of difference we are willing to accept to count as good prediction\n",
    "\n",
    "start = timer()\n",
    "vect_dict = loadGloveModel(GLOVE_PATH)\n",
    "end = timer()\n",
    "elapsed_sec = end - start # Time in seconds, e.g. 5.38091952400282\n",
    "print(\"Finished loading vect_dict in\", elapsed_sec, \"seconds \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Bunch of Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running train() .... \n",
      "\n",
      "Progress  0.0625782227784731 %\r"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "print(\"Running train() .... \\n\")\n",
    "start = timer()\n",
    "\n",
    "result_list = train(vect_dict,50) # (predicted_value - actual_value, the index at where this difference occurred)\n",
    "\n",
    "end = timer()\n",
    "elapsed_sec = end - start # Time in seconds, e.g. 5.38091952400282\n",
    "print(\"Finished running train() in \", elapsed_sec, \"seconds \\n\")\n",
    "print(\"Number of tests: \",len(result_list))\n",
    "print(\"margin 0, margin, .25, margin 1, average difference\")\n",
    "print(calculate_accuracy(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6551939010620114, 79), (0.44582605361938477, 308), (0.5445232391357422, 900), (0.23077554702758807, 120), (0.6381503105163571, 391), (0.5682649612426758, 770), (0.5491167068481442, 578), (2.2450206756591795, 272), (0.1933332443237301, 383), (0.46863269805908203, 952), (0.26123123168945295, 373), (1.027162170410156, 91), (0.36696472167968786, 657), (1.3579700469970701, 100), (0.46851291656494176, 293), (0.5483079910278317, 215), (0.1386757850646969, 755), (0.8194864273071287, 126), (1.6463609695434567, 74), (0.5691467285156246, 869), (0.13782844543456996, 895), (0.05688819885253871, 673), (0.29238910675048846, 87), (0.4293251037597656, 810), (1.3651540756225584, 176), (0.570793628692627, 105), (1.0295343399047852, 506), (0.24357481002807635, 538), (0.5229434967041016, 38), (0.5623370170593258, 664), (0.6758796691894533, 632), (1.3389885902404783, 722), (0.9247879028320316, 116), (0.7435724258422853, 417), (2.1441942214965817, 329), (0.738052558898926, 671), (0.32161216735839826, 581), (1.5431385040283203, 25), (1.3248374938964842, 180), (0.6655069351196286, 626), (0.06795892715454066, 535), (1.650176620483398, 583), (1.2252990722656252, 954), (1.934045219421387, 628), (0.2541749954223631, 751), (0.2672031402587889, 334), (0.03699302673339844, 969), (0.33272542953491246, 278), (1.2153293609619142, 127), (0.778081703186035, 485), (0.5374083518981934, 681), (0.06658229827880824, 605), (0.3455741882324217, 438), (0.6241765975952145, 763), (0.6407991409301754, 449), (1.5198721885681152, 46), (0.6310440063476559, 60), (0.46988964080810547, 990), (0.17218370437622088, 40), (0.9535154342651371, 545), (0.7431337356567385, 713), (0.12656803131103533, 799), (0.9504512786865238, 891), (0.0703439712524414, 437), (0.43779363632202184, 294), (1.0603357315063473, 789), (1.9268373489379886, 298), (0.5513749122619629, 639), (0.4608125686645508, 254), (1.2573406219482424, 904), (0.925213718414307, 407), (0.7377984046936037, 663), (0.7297832489013674, 137), (1.3516142845153807, 343), (0.23005847930908185, 505), (0.048714637756347656, 786), (0.44086399078369176, 570), (0.5432510375976562, 728), (1.0355257987976074, 941), (0.06985244750976527, 550), (1.737545680999756, 883), (0.23130626678466815, 580), (0.014551639556884766, 660), (0.025777816772460938, 933), (2.0657029151916504, 330), (0.661735248565674, 670), (0.5557733535766598, 619), (1.5246996879577637, 119), (0.7432071685791017, 358), (1.340165996551514, 608), (0.2514407157897951, 508), (0.9366697311401371, 528), (0.5273590087890625, 446), (0.12604579925537074, 403), (0.5623569488525391, 128), (0.44202461242675817, 815), (0.32963876724243146, 429), (0.029484272003173828, 762), (0.6611147880554196, 769), (0.22359857559204066, 405), (0.4370740890502933, 655), (0.2609617233276369, 973), (0.9436501502990726, 513), (0.4385198593139652, 90), (0.045827388763427734, 746), (0.05891380310058558, 168), (0.12786731719970668, 696), (1.5700946807861325, 965), (0.17259187698364276, 807), (0.43266906738281286, 123), (1.1220475196838375, 650), (0.23537559509277362, 720), (1.0655813217163086, 842), (0.4658311843872074, 433), (1.074192142486572, 555), (0.04646883010864222, 419), (0.2672050476074217, 795), (1.747853088378906, 303), (1.843152332305908, 287), (0.08125791549682582, 996), (1.0386686325073242, 928), (0.13467226028442347, 253), (0.2706878662109373, 486), (0.01646900177001953, 590), (0.25106697082519513, 53), (1.123853778839111, 909), (0.16782970428466815, 574), (1.9386075973510746, 351), (0.060634613037109375, 936), (0.1272908210754391, 958), (0.7422732353210453, 393), (0.8691190719604496, 70), (0.7676508903503416, 451), (0.14542541503906214, 174), (0.19008274078369158, 436), (0.9372371673583988, 617), (1.8530892372131351, 499), (0.5643344879150387, 10), (0.15481252670288104, 586), (0.7612913131713865, 458), (0.7475034713745119, 326), (0.14089164733886683, 468), (0.37227191925048864, 898), (0.24515323638915998, 544), (0.26209754943847674, 214), (0.6689640045166012, 301), (0.8542751312255863, 41), (0.04145393371581996, 988), (1.6444054603576657, 270), (1.3462093353271483, 635), (1.5534711837768551, 242), (0.06162223815917933, 43), (1.03485107421875, 34), (1.035090446472168, 182), (0.15994091033935565, 753), (0.6378676414489748, 492), (0.8027531623840334, 300), (0.7607849121093748, 491), (0.2688629150390627, 858), (0.06283054351806605, 121), (0.13926563262939418, 729), (0.5256891250610352, 921), (0.0869165420532223, 227), (0.07064065933227504, 733), (2.030588722229004, 503), (0.34030761718750036, 157), (0.5505828857421875, 310), (0.036100959777831676, 443), (0.93944206237793, 561), (0.4229120254516605, 708), (1.045724868774414, 852), (0.2379100799560545, 332), (0.1220727920532223, 4), (0.022346973419189453, 968), (0.364392185211182, 907), (0.3490951538085936, 33), (0.055895900726318004, 611), (0.1701871871948244, 23), (0.45680952072143555, 369), (0.5300607681274414, 302), (0.37005987167358434, 362), (0.2613780975341795, 707), (0.36749877929687536, 995), (0.638748359680176, 440), (0.7770908355712889, 479), (0.08957538604736293, 599), (0.38784866333007795, 336), (0.5262384414672852, 313), (0.034850120544433594, 646), (1.6138120651245114, 160), (0.4237517356872562, 64), (0.6619011878967287, 715), (0.7620827674865724, 878), (1.843530464172363, 88), (0.16485271453857386, 273), (0.14180669784545863, 855), (0.6355348587036129, 695), (1.149908638000488, 207), (0.23852176666259783, 747)]\n"
     ]
    }
   ],
   "source": [
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
