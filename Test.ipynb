{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from timeit import default_timer as timer # For timing runtime of function\n",
    "from torch.autograd import Variable\n",
    "#source: https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "\n",
    "#This function takes in the path to the glove 50d pretrained vectors that is\n",
    "#originally stored as a .txt and then converts it into a dictionary. To access\n",
    "#a certain vector of a certain word, use the word as the key, and the dictionary\n",
    "#will return the vector.\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',encoding=\"utf-8\")\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "#This function takes in the path to our modified csv file and returns individual\n",
    "#lists containing title, summary, and score, respectively. \n",
    "#\n",
    "#Example: title[k] returns the movie at index k's title in the form of a string\n",
    "#\n",
    "#@ Params: csvFile - the path to our modified csv file containing only the usable\n",
    "#                    movies\n",
    "#\n",
    "#@ Return: title - a list of all the movie titles inside of csvFile\n",
    "#          summary - a list of all the summaries inside of csvFile\n",
    "#          score - a list of all the scores (voter averages) inside of csvFile\n",
    "def parseCSV(csvFile):\n",
    "    #define the column names\n",
    "    \n",
    "    #reading the file to parse\n",
    "    with open(csvFile, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        read_list = list(reader)\n",
    "    \n",
    "    #stores the respective data in lists\n",
    "    title = []\n",
    "    summary = []\n",
    "    score = []\n",
    "    \n",
    "    # To parse all movies, set range(1, len(read_list))\n",
    "    for i in range(1, 50):\n",
    "        title.append(read_list[i][0])\n",
    "        summary.append(read_list[i][1])\n",
    "        score.append(float(read_list[i][2]))\n",
    "    #because the first index of each list is the header, we get rid of\n",
    "    #the first element in each list\n",
    "\n",
    "    return title, summary, score\n",
    "\n",
    "#This function takes in the summary list and then randomly generates a test\n",
    "#set and a training set. The hardcoded value is 80 percent training and 20\n",
    "#percent test. This returns a trainingSet, a testSet, a testSetIndex, and\n",
    "#a trainingSetIndex list. \n",
    "#\n",
    "#@ Params: summary - the summary list generated by the function parseCSV\n",
    "#@ Return: trainingSet - the summaries inside of the parameter summary to be used\n",
    "#                        as training data\n",
    "#          testSet - the summaries inside of the parameter summary to be used for\n",
    "#                    testing purposes\n",
    "#          testSetIndex - a list of integers that represent which indices inside\n",
    "#                         summary are being used for testing purposes\n",
    "#          trainingSetIndex - a list of integers that represent which indices inside\n",
    "#                             summary are being used for training purposes.\n",
    "def generateSets(summary):\n",
    "    #definition of constants throughout this function\n",
    "    training_percentage = .8\n",
    "    list_size = len(summary) - 1\n",
    "    \n",
    "    #initialize the things to return\n",
    "    testSetIndex = []\n",
    "    testSet = []\n",
    "    trainingSet = []\n",
    "    trainingSetIndex = []\n",
    "    \n",
    "    #Steps to generate the test set:\n",
    "    #1. Randomly generate an integer between 0 and list_size\n",
    "    #2. Check to see if this index is already within our test set\n",
    "    #3. If it isn't, document this index inside our testSetIndex and go on\n",
    "    #4. If it is, generate another random number until we find one that isn't\n",
    "    #5. Repeat until testSetIndex is of size (1-training_percentage)\n",
    "    for i in range(int(list_size * (1-training_percentage))):\n",
    "        rand_index = random.randint(0, list_size)\n",
    "        \n",
    "        #keep generating until it is NOT inside of testSetIndex\n",
    "        while(rand_index in testSetIndex):\n",
    "            rand_index = random.randint(0, list_size)\n",
    "            \n",
    "        testSetIndex.append(rand_index)\n",
    "        testSet.append(summary[rand_index])\n",
    "        \n",
    "    #After this, we will have int[(1-training_percentage)*list_size] elements\n",
    "    #inside of testSet. Training set is everything else. \n",
    "    for k in range(list_size):\n",
    "        \n",
    "        if(k not in testSetIndex):\n",
    "            trainingSet.append(summary[k])\n",
    "            trainingSetIndex.append(k)\n",
    "            \n",
    "    return trainingSet, testSet, testSetIndex, trainingSetIndex\n",
    "\n",
    "\n",
    "#taken and modified from: \n",
    "#https://stackoverflow.com/questions/9797357/dividing-a-string-at-various-punctuation-marks-using-split\n",
    "\n",
    "#This function splits the sentence by spaces and punctuation marks. Punctuation\n",
    "#marks refer to non-alphanumeric characters that can appear validly inside\n",
    "#of the English sentence. One exception to this rule is the string \"'s\" as this\n",
    "#has it's own vector inside of the glove pretrained vector set. Therefore, this\n",
    "#particular string will get it's own index. \n",
    "#@Param: text- the text to be parsed. This must a valid English block of text,\n",
    "#              as in, it must be readable to the everyday person\n",
    "#@Retuen: sentence - a list containing the parsed version of the input text.\n",
    "#                    Each index of this will contain either a punctuation mark\n",
    "#                    by itself, the string \"'s\" by itself, or an English word.\n",
    "def parseSentence(text):\n",
    "    sentence = (\"\".join((char if char.isalnum() else (\" \"+ char + \" \")) for char in text).split())\n",
    "    \n",
    "    #join the instances of \"'s\" and ONLY \"'s\"\n",
    "    i = 0\n",
    "    while i in range (len(sentence)):\n",
    "        if(sentence[i] == \"'\" and i+1 in range(len(sentence)) and sentence[i+1] == 's'):\n",
    "            sentence[i] = \"'s\"\n",
    "            sentence.pop(i+1)\n",
    "        i += 1\n",
    "            \n",
    "    #join the rest of the punctuation marks. AKA non-alphanumeric characters\n",
    "    j = 0\n",
    "    while j in range (len(sentence)): \n",
    "        if(sentence[j] != \"'s\" and not sentence[j].isalnum()):\n",
    "            k = j+1\n",
    "            while (k in range (len(sentence)) and not sentence[k].isalnum()):\n",
    "                sentence[j] = sentence[j] + sentence[k]\n",
    "                sentence.pop(k)\n",
    "        \n",
    "        j += 1\n",
    "    \n",
    "    #Because the glove database is all lowercase, we need to lowercase everything\n",
    "    for k in range(len(sentence)):\n",
    "        sentence[k] = sentence[k].lower()\n",
    "    \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "    \n",
    "def LSTM(vec_dict):\n",
    "    model = vec_dict\n",
    "    title, summary, score = parseCSV(MOVIE_CSV)\n",
    "    trainingSet, testSet, testSetIndex, trainingSetIndex = generateSets(summary)\n",
    "    \n",
    "    inputs = []\n",
    "    #for each summary inside of out trainingSet\n",
    "    for i in range(len(trainingSet)):\n",
    "        parsed_summary = parseSentence(trainingSet[i])\n",
    "        \n",
    "        #For each word, punctuation, or instance of 's\n",
    "        for j in range(len(parsed_summary)):\n",
    "            #Make sure the string exists inside of the glove model\n",
    "            if(parsed_summary[j] in model):\n",
    "                inputs.append(torch.from_numpy(model[parsed_summary[j]]).float())\n",
    "\n",
    "    torch.manual_seed(1)            \n",
    "    lstm = nn.LSTM(50, 50)\n",
    "    \n",
    "    hidden = (torch.randn(1, 1, 50), torch.randn(1, 1, 50))\n",
    "    \n",
    "    for k in inputs:\n",
    "        out, hidden = lstm(k.view(1, 1,-1), hidden)\n",
    "        \n",
    "    return hidden[0]\n",
    "\n",
    "    \n",
    "# https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html\n",
    "# https://www.kaggle.com/pinocookie/pytorch-simple-mlp\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(50, 40),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(40, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 20),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\"\"\"LSTM and MLP working\n",
    "\n",
    "Added: train(vec_dict), vec_dict is the loaded glove embeddings. train contains our LSTM (defined inside function) and \n",
    "       MLP (defined outside of function, see the class definition of MLP right above the train function). \n",
    "\n",
    "What this function does:\n",
    "\n",
    "1) For each of the summaries inside trainingSet, it inputs each word embedding inside of a summary into the LSTM\n",
    "   with randomized inital hidden states.\n",
    "\n",
    "2) After the LSTM returns the final hidden state, it inputs the final hidden state into the MLP\n",
    "\n",
    "3) Using what the MLP returns and the true vote_average for a certain movie, \n",
    "   a loss function is used to calculate the loss. This loss function can be changed,\n",
    "   I chose one arbitrarily. Performs gradient descent.\n",
    "\n",
    "4) After training, it goes through the testSet and uses the same LSTM (with randomized inital hidden states) \n",
    "   and the same MLP to obtain a predicted value.\n",
    "\n",
    "5) This data is stored inside a list. The list differences contain the tuple:\n",
    "   (predicted_value - actual_value, the index at where this difference occurred)\n",
    "\n",
    "TODO: We need to define when we deem a prediction to be \"accurate\". We need to represent the data in a meaningful way(histogram? trainingPercentage vs accuracy?)\n",
    "\n",
    "NOTE: I am only using the first 2000 movies for the sake of speed during debugging. To use the entire movie list:\n",
    "\n",
    "Go to parseCSV function, inside for loop change 2000 -> len(read_list)\n",
    "\"\"\"\n",
    "def train(vec_dict):\n",
    "    #define our constants\n",
    "    vec_model = vec_dict\n",
    "    mlp_model = MLP()\n",
    "    optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    title, summary, score = parseCSV(MOVIE_CSV)\n",
    "    trainingSet, testSet, testSetIndex, trainingSetIndex = generateSets(summary)\n",
    "    torch.manual_seed(1)\n",
    "    lstm = nn.LSTM(50,50)\n",
    "   \n",
    "    \n",
    "######################### START TRAINING ######################################\n",
    "    \n",
    "    #For each training set\n",
    "    for i in range(len(trainingSet)):\n",
    "        parsed_summary = parseSentence(trainingSet[i])\n",
    "        associated_index = trainingSetIndex[i]\n",
    "        inputs = []\n",
    "        \n",
    "        #populate the input list with our word embeddings\n",
    "        for j in range(len(parsed_summary)):\n",
    "            #Make sure the string exists inside of the glove model\n",
    "            if(parsed_summary[j] in vec_model):\n",
    "                inputs.append(torch.from_numpy(vec_model[parsed_summary[j]]).float())\n",
    "                \n",
    "        #initialize hidden states\n",
    "        hidden = (torch.randn(1, 1, 50), torch.randn(1, 1, 50))\n",
    "        \n",
    "        #for each index inside of our inputs, we input it into the lstm sequentially\n",
    "        for k in inputs:\n",
    "            out, hidden = lstm(k.view(1, 1,-1), hidden)\n",
    "            \n",
    "        #we do not use the cell state, so we need to call hidden[0] when inputting\n",
    "        #into MLP\n",
    "        predicted_value = mlp_model(hidden[0])\n",
    "        \n",
    "        #use associated_index to find the true value inside of our score list\n",
    "        actual_value = torch.tensor(score[associated_index], requires_grad=True)\n",
    "        \n",
    "        #clears the gradient as Pytorch accumulates gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #find our total loss\n",
    "        loss = loss_fn(predicted_value, actual_value)\n",
    "        \n",
    "        #gradient descent\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "######################### END TRAINING #######################################\n",
    "    \n",
    "######################### START TESTING ######################################\n",
    "    \n",
    "    #This list will store the difference between our predicted value vs actual\n",
    "    differences = []\n",
    "    \n",
    "    #For each test set\n",
    "    for i in range(len(testSet)):\n",
    "        parsed_summary = parseSentence(testSet[i])\n",
    "        associated_index = testSetIndex[i]\n",
    "        inputs = []\n",
    "        \n",
    "        #populate the input list with our word embeddings\n",
    "        for j in range(len(parsed_summary)):\n",
    "            #Make sure the string exists inside of the glove model\n",
    "            if(parsed_summary[j] in vec_model):\n",
    "                inputs.append(torch.from_numpy(vec_model[parsed_summary[j]]).float())\n",
    "                \n",
    "        #initialize hidden states\n",
    "        hidden = (torch.randn(1, 1, 50), torch.randn(1, 1, 50))\n",
    "        \n",
    "        #for each index inside of our inputs, we input it into the lstm sequentially\n",
    "        for k in inputs:\n",
    "            out, hidden = lstm(k.view(1, 1,-1), hidden)\n",
    "        \n",
    "        #we do not use the cell state, so we need to call hidden[0] when inputting\n",
    "        #into MLP\n",
    "        predicted_value = mlp_model(hidden[0]).item()\n",
    "        actual_value = score[associated_index]\n",
    "        #create a tuple (difference of actual vs pred, index of movie)\n",
    "        \n",
    "        temp_tuple = (predicted_value - actual_value, associated_index)\n",
    "        differences.append(temp_tuple)\n",
    "        \n",
    "    return differences\n",
    "######################### END TESTING ######################################\n",
    "    \n",
    "def calculate_accuracy(differences):\n",
    "    size = len(differences)\n",
    "    num_correct = 0\n",
    "    for element in differences:\n",
    "        diff = element[0]\n",
    "        if diff <= MARGIN_ERROR:\n",
    "            num_correct += num_correct\n",
    "    return num_correct/size\n",
    "        \n",
    "def yolo():\n",
    "    print(\"yolo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n",
      "Running train() .... \n",
      "\n",
      "Finished running train() in  5.717856443490746 seconds \n",
      "\n",
      "Number of tests:  9 \n",
      " [(-5.3791069984436035, 35), (-5.780598664283753, 32), (-5.28012444972992, 5), (-6.878690123558044, 46), (-6.979372715950012, 42), (-5.879568338394165, 38), (-6.779308104515076, 47), (-5.479766941070556, 4), (-5.681228387355804, 33)]\n"
     ]
    }
   ],
   "source": [
    "# Define path variables\n",
    "GLOVE_PATH= './glove.6B.50d.txt'\n",
    "MOVIE_CSV = 'tmdb_5000_movies_modified.csv'\n",
    "MARGIN_ERROR = 1 # How large of difference we are willing to accept to count as good prediction\n",
    "\n",
    "vect_dict = loadGloveModel(GLOVE_PATH)\n",
    "\n",
    "print(\"Running train() .... \\n\")\n",
    "start = timer()\n",
    "\n",
    "result_list = train(vect_dict) # (predicted_value - actual_value, the index at where this difference occurred)\n",
    "\n",
    "end = timer()\n",
    "elapsed_sec = end - start # Time in seconds, e.g. 5.38091952400282\n",
    "print(\"Finished running train() in \", elapsed_sec, \"seconds \\n\")\n",
    "print(\"Number of tests: \",len(result_list), \"\\n\", result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(calculate_accuracy(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
