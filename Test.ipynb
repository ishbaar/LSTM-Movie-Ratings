{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#source: https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "\n",
    "#This function takes in the path to the glove pretrained vectors that is\n",
    "#originally stored as a .txt and then converts it into a dictionary. To access\n",
    "#a certain vector of a certain word, use the word as the key, and the dictionary\n",
    "#will return the vector.\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',encoding=\"utf-8\")\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "#This function takes in the path to our modified csv file and returns individual\n",
    "#lists containing title, summary, and score, respectively. \n",
    "#\n",
    "#Example: title[k] returns the movie at index k's title in the form of a string\n",
    "#\n",
    "#@ Params: csvFile - the path to our modified csv file containing only the usable\n",
    "#                    movies\n",
    "#\n",
    "#@ Return: title - a list of all the movie titles inside of csvFile\n",
    "#          summary - a list of all the summaries inside of csvFile\n",
    "#          score - a list of all the scores (voter averages) inside of csvFile\n",
    "def parseCSV(csvFile):\n",
    "    #define the column names\n",
    "    \n",
    "    #reading the file to parse\n",
    "    with open(csvFile, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        read_list = list(reader)\n",
    "    \n",
    "    #stores the respective data in lists\n",
    "    title = []\n",
    "    summary = []\n",
    "    score = []\n",
    "    \n",
    "    # len(read_list)\n",
    "    for i in range(1, len(read_list)):\n",
    "        title.append(read_list[i][0])\n",
    "        summary.append(read_list[i][1])\n",
    "        score.append(float(read_list[i][2]))\n",
    "    #because the first index of each list is the header, we get rid of\n",
    "    #the first element in each list\n",
    "\n",
    "    \n",
    "    return title, summary, score\n",
    "\n",
    "#This function takes in the summary list and then randomly generates a test\n",
    "#set and a training set. The hardcoded value is 80 percent training and 20\n",
    "#percent test. This returns a trainingSet, a testSet, a testSetIndex, and\n",
    "#a trainingSetIndex list. \n",
    "#\n",
    "#@ Params: summary - the summary list generated by the function parseCSV\n",
    "#@ Return: trainingSet - the summaries inside of the parameter summary to be used\n",
    "#                        as training data\n",
    "#          testSet - the summaries inside of the parameter summary to be used for\n",
    "#                    testing purposes\n",
    "#          testSetIndex - a list of integers that represent which indices inside\n",
    "#                         summary are being used for testing purposes\n",
    "#          trainingSetIndex - a list of integers that represent which indices inside\n",
    "#                             summary are being used for training purposes.\n",
    "def generateSets(summary):\n",
    "    #definition of constants throughout this function\n",
    "    training_percentage = .8\n",
    "    list_size = len(summary) - 1\n",
    "    \n",
    "    #initialize the things to return\n",
    "    testSetIndex = []\n",
    "    testSet = []\n",
    "    trainingSet = []\n",
    "    trainingSetIndex = []\n",
    "    \n",
    "    #Steps to generate the test set:\n",
    "    #1. Randomly generate an integer between 0 and list_size\n",
    "    #2. Check to see if this index is already within our test set\n",
    "    #3. If it isn't, document this index inside our testSetIndex and go on\n",
    "    #4. If it is, generate another random number until we find one that isn't\n",
    "    #5. Repeat until testSetIndex is of size (1-training_percentage)\n",
    "    for i in range(int(list_size * (1-training_percentage))):\n",
    "        rand_index = random.randint(0, list_size)\n",
    "        \n",
    "        #keep generating until it is NOT inside of testSetIndex\n",
    "        while(rand_index in testSetIndex):\n",
    "            rand_index = random.randint(0, list_size)\n",
    "            \n",
    "        testSetIndex.append(rand_index)\n",
    "        testSet.append(summary[rand_index])\n",
    "        \n",
    "    #After this, we will have int[(1-training_percentage)*list_size] elements\n",
    "    #inside of testSet. Training set is everything else. \n",
    "    for k in range(list_size):\n",
    "        \n",
    "        if(k not in testSetIndex):\n",
    "            trainingSet.append(summary[k])\n",
    "            trainingSetIndex.append(k)\n",
    "            \n",
    "    return trainingSet, testSet, testSetIndex, trainingSetIndex\n",
    "\n",
    "#taken and modified from: \n",
    "#https://stackoverflow.com/questions/9797357/dividing-a-string-at-various-punctuation-marks-using-split\n",
    "\n",
    "#This function splits the sentence by spaces and punctuation marks. Punctuation\n",
    "#marks refer to non-alphanumeric characters that can appear validly inside\n",
    "#of the English sentence. One exception to this rule is the string \"'s\" as this\n",
    "#has it's own vector inside of the glove pretrained vector set. Therefore, this\n",
    "#particular string will get it's own index. \n",
    "#\n",
    "#@Param: text- the text to be parsed. This must a valid English block of text,\n",
    "#              as in, it must be readable to the everyday person\n",
    "#@Retuen: sentence - a list containing the parsed version of the input text.\n",
    "#                    Each index of this will contain either a punctuation mark\n",
    "#                    by itself, the string \"'s\" by itself, or an English word.\n",
    "def parseSentence(text):\n",
    "    sentence = (\"\".join((char if char.isalnum() else (\" \"+ char + \" \")) for char in text).split())\n",
    "    \n",
    "    #join the instances of \"'s\" and ONLY \"'s\"\n",
    "    i = 0\n",
    "    while i in range (len(sentence)):\n",
    "        if(sentence[i] == \"'\" and i+1 in range(len(sentence)) and sentence[i+1] == 's'):\n",
    "            sentence[i] = \"'s\"\n",
    "            sentence.pop(i+1)\n",
    "        i += 1\n",
    "            \n",
    "    #join the rest of the punctuation marks. AKA non-alphanumeric characters\n",
    "    j = 0\n",
    "    while j in range (len(sentence)): \n",
    "        if(sentence[j] != \"'s\" and not sentence[j].isalnum()):\n",
    "            k = j+1\n",
    "            while (k in range (len(sentence)) and not sentence[k].isalnum()):\n",
    "                sentence[j] = sentence[j] + sentence[k]\n",
    "                sentence.pop(k)\n",
    "        \n",
    "        j += 1\n",
    "    \n",
    "    #Because the glove database is all lowercase, we need to lowercase everything\n",
    "    for k in range(len(sentence)):\n",
    "        sentence[k] = sentence[k].lower()\n",
    "    \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "#Definition of our MLP model. The input layer has input size of glove_dim which\n",
    "#is the dimensionality of the GloVe dataset we are currently using. This has 5\n",
    "#hidden layers, all with 100 nodes and a output layer of 1, which is our predicted\n",
    "#user rating(voter average)\n",
    "#\n",
    "#@Param: glove_dim - the dimensionality of the GloVe dataset we are currently using\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, glove_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            #Input layer: dimension of GloVe embedding we are currently using\n",
    "            nn.Linear(glove_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            #output layer: 1\n",
    "            nn.Linear(100, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "#This function defines a LSTM and a MLP and trains the MLP in order to generate\n",
    "#predicted ratings for a given summary. It takes in the dictionary that contains\n",
    "#all the GloVe pre-trained word embeddings. It first calls parseCSV to parse the\n",
    "#CSV file containing all the movies. Then it calls generateSets to define our\n",
    "#test and training sets. For each summary inside of our training set, it feeds\n",
    "#the summary into the LSTM and then uses the final hidden state as the vector\n",
    "#representation for the entire summary. The MLP takes in this vector and generates\n",
    "#a predicted rating. We perform SGD on the loss function in order to train the MLP.\n",
    "#After training, for every test set, the MLP predicts a rating and then the\n",
    "#difference is appended to a list and the list is returned after going through\n",
    "#all the test sets.\n",
    "#\n",
    "#@Param: vec_dict - the dictonary containing the GloVe pre-trained vector representations\n",
    "#                   of words\n",
    "#        vec_dict_dim - the dimensionality of the GloVe dataset you are using.\n",
    "#                       Should be one of the following numbers: 50, 100, 200, 300\n",
    "#@Return: differences - A list containing tuples of:\n",
    "#                       (abs(predicted_value - actual_value),\n",
    "#                       the index of where this difference occured)\n",
    "def train(vec_dict, vec_dict_dim):\n",
    "    #Define our constants\n",
    "    vec_model = vec_dict\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    title, summary, score = parseCSV('tmdb_5000_movies_modified.csv')\n",
    "    trainingSet, testSet, testSetIndex, trainingSetIndex = generateSets(summary)\n",
    "    torch.manual_seed(1)\n",
    "    \n",
    "\n",
    "    glove_dim = vec_dict_dim\n",
    "    \n",
    "    #Define our models \n",
    "    mlp_model = MLP(glove_dim)\n",
    "    optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.001)\n",
    "    lstm = nn.LSTM(glove_dim,glove_dim)\n",
    "   \n",
    "    \n",
    "######################### START TRAINING ######################################\n",
    "    \n",
    "    #For each training set\n",
    "    for i in range(len(trainingSet)):\n",
    "        parsed_summary = parseSentence(trainingSet[i])\n",
    "        associated_index = trainingSetIndex[i]\n",
    "        inputs = []\n",
    "        \n",
    "        #populate the input list with our word embeddings\n",
    "        for j in range(len(parsed_summary)):\n",
    "            #Make sure the string exists inside of the glove model\n",
    "            if(parsed_summary[j] in vec_model):\n",
    "                inputs.append(torch.from_numpy(vec_model[parsed_summary[j]]).float())\n",
    "                \n",
    "        #initialize hidden states\n",
    "        hidden = (torch.ones(1, 1, glove_dim), torch.ones(1, 1, glove_dim))\n",
    "        \n",
    "        #for each index inside of our inputs, we input it into the lstm sequentially\n",
    "        for k in inputs:\n",
    "            out, hidden = lstm(k.view(1, 1,-1), hidden)\n",
    "            \n",
    "        #we do not use the cell state, so we need to call hidden[0] when inputting\n",
    "        #into MLP\n",
    "        predicted_value = mlp_model(hidden[0])\n",
    "        \n",
    "        #use associated_index to find the true value inside of our score list\n",
    "        actual_value = torch.tensor(score[associated_index], requires_grad=True)\n",
    "        \n",
    "        #clears the gradient as Pytorch accumulates gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #find our total loss\n",
    "        loss = loss_fn(predicted_value, actual_value)\n",
    "        \n",
    "        #gradient descent\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i % 50 == 0):\n",
    "            print(\"Progress\", \"{0:.2f}\".format((i*100) / len(trainingSet)), \"%\", end=\"\\r\")\n",
    "            \n",
    "        \n",
    "######################### END TRAINING #######################################\n",
    "    \n",
    "######################### START TESTING ######################################\n",
    "    \n",
    "    #This list will store the difference between our predicted value vs actual\n",
    "    differences = []\n",
    "    #For each test set\n",
    "    for i in range(len(testSet)):\n",
    "        parsed_summary = parseSentence(testSet[i])\n",
    "        associated_index = testSetIndex[i]\n",
    "        inputs = []\n",
    "        \n",
    "        #populate the input list with our word embeddings\n",
    "        for j in range(len(parsed_summary)):\n",
    "            #Make sure the string exists inside of the glove model\n",
    "            if(parsed_summary[j] in vec_model):\n",
    "                inputs.append(torch.from_numpy(vec_model[parsed_summary[j]]).float())\n",
    "                \n",
    "        #initialize hidden states\n",
    "        hidden = (torch.ones(1, 1, glove_dim), torch.ones(1, 1, glove_dim))\n",
    "        \n",
    "        #for each index inside of our inputs, we input it into the lstm sequentially\n",
    "        for k in inputs:\n",
    "            out, hidden = lstm(k.view(1, 1,-1), hidden)\n",
    "            \n",
    "        #we do not use the cell state, so we need to call hidden[0] when inputting\n",
    "        #into MLP\n",
    "        predicted_value = mlp_model(hidden[0]).item()\n",
    "        actual_value = score[associated_index]\n",
    "\n",
    "        temp_tuple = (abs(predicted_value - actual_value), associated_index)\n",
    "        differences.append(temp_tuple)\n",
    "        \n",
    "######################### END TESTING ########################################\n",
    "        \n",
    "    return differences\n",
    "\n",
    "#This function takes in the differences list from the train() function and it\n",
    "#outputs the accuracy of the testing, the maximum difference, and the index at\n",
    "#where this maximum difference occured.\n",
    "#\n",
    "#@Param: differences - the list returned by the function train(). This should\n",
    "#                      be a list containing tuples of the form:\n",
    "#                      (abs(predicted_value - actual_value), \n",
    "#                       index at where this difference occured)\n",
    "#@Return: final_accuracy - A float that represents the accuracy of our testing\n",
    "#         max_diff - The maximum difference inside of the list difference\n",
    "#         max_diff_index - the index at where max_diff occurs\n",
    "def calculate_accuracy(differences):\n",
    "    #initialize the differences to the first element\n",
    "    max_diff = differences[0][0]\n",
    "    max_diff_index = differences[0][1]\n",
    "\n",
    "    #initialize our constant\n",
    "    size = len(differences)\n",
    "    \n",
    "    #initialize our counter\n",
    "\n",
    "    num_correct_0 = 0\n",
    "    num_correct_25 = 0\n",
    "    num_correct_5 = 0\n",
    "    num_correct_1 = 0\n",
    "    \n",
    "    #definition of what is \"accurate\". Anything <= MARGIN_ERROR is deemed accurate.\n",
    "    MARGIN_ERROR = .25\n",
    "    \n",
    "    #iterate through the list of differences\n",
    "    for element in differences:\n",
    "        \n",
    "        #check for maximum difference\n",
    "        if element[0] > max_diff:\n",
    "            max_diff = element[0]\n",
    "            max_diff_index = element[1]\n",
    "            \n",
    "        #this gets the difference in predicted vs actual\n",
    "        diff = element[0]\n",
    "        \n",
    "        if diff <= .25:\n",
    "            num_correct_25 += 1\n",
    "        if diff == 0:\n",
    "            num_correct_0 += 1\n",
    "        if diff <= .5:\n",
    "           num_correct_5 += 1\n",
    "        if diff <= 1:\n",
    "            num_correct_1 += 1\n",
    "\n",
    "    \n",
    "    #accuracy = (accurate predictions / size of list)\n",
    "    final_accuracy_0 = num_correct_0/size\n",
    "    final_accuracy_25 = num_correct_25/size\n",
    "    final_accuracy_5 = num_correct_5/size\n",
    "    final_accuracy_1 = num_correct_1/size\n",
    "    \n",
    "    array = np.array(differences)\n",
    "    return final_accuracy_0, final_accuracy_25,final_accuracy_5, final_accuracy_1, round(np.average(array,axis=0)[0],3)  # , max_diff, max_diff_index\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n",
      "Finished loading vect_dict in 14.713267134231355 seconds \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path variables\n",
    "GLOVE_PATH= './glove.6B.50d.txt'\n",
    "MOVIE_CSV = 'tmdb_5000_movies_modified.csv'\n",
    "MARGIN_ERROR = 1 # How large of difference we are willing to accept to count as good prediction\n",
    "\n",
    "start = timer()\n",
    "vect_dict = loadGloveModel(GLOVE_PATH)\n",
    "end = timer()\n",
    "elapsed_sec = end - start # Time in seconds, e.g. 5.38091952400282\n",
    "print(\"Finished loading vect_dict in\", elapsed_sec, \"seconds \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Bunch of Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running train() .... \n",
      "\n",
      "margin 0, margin, .25, margin .5 margin 1, average difference\n",
      "(0.0, 0.22597676874340022, 0.41816261879619854, 0.7106652587117213, 0.759)\n",
      "(0.0, 0.21647307286166842, 0.4033790918690602, 0.7096092925026399, 0.774)\n",
      "(0.0, 0.20802534318901794, 0.40971488912354803, 0.7001055966209081, 0.77)\n",
      "(0.0, 0.21119324181626187, 0.4192185850052798, 0.7296726504751848, 0.75)\n",
      "(0.0, 0.21647307286166842, 0.40549102428722283, 0.7074973600844773, 0.764)\n",
      "Finished running train() in  1732.6820447935934 seconds \n",
      "\n",
      "Number of tests:  947\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "print(\"Running train() .... \\n\")\n",
    "start = timer()\n",
    "\n",
    "print(\"margin 0, margin, .25, margin .5 margin 1, average difference\")\n",
    "for i in range(0,5):\n",
    "    result_list = train(vect_dict,50) # (predicted_value - actual_value, the index at where this difference occurred)\n",
    "    print(calculate_accuracy(result_list))\n",
    "\n",
    "\n",
    "end = timer()\n",
    "elapsed_sec = end - start # Time in seconds, e.g. 5.38091952400282\n",
    "print(\"Finished running train() in \", elapsed_sec, \"seconds \\n\")\n",
    "print(\"Number of tests: \",len(result_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.04676160812377894, 72), (0.6944161415100094, 129), (0.261024761199951, 552), (0.03531847000122035, 318), (0.24917917251586896, 645), (0.7661668777465822, 326), (0.5672202110290527, 248), (0.24793081283569318, 9), (0.47242059707641637, 474), (0.08882951736450195, 488), (0.5497303962707516, 869), (0.6482645988464357, 705), (1.9735659599304203, 634), (0.7537967681884767, 730), (0.9493054389953617, 181), (0.23553447723388654, 224), (0.8555500984191893, 390), (0.5705685615539551, 524), (0.38662748336791974, 773), (0.2318656921386717, 53), (1.3706443786621092, 343), (1.2720825195312502, 912), (0.4417523384094242, 708), (0.2132426261901852, 383), (0.7628609657287599, 417), (0.7565648078918459, 671), (0.7627913475036623, 475), (0.1541954994201662, 807), (1.1146456718444826, 668), (0.04058504104614258, 424), (0.8642809867858885, 930), (0.41985559463500977, 132), (1.7619869232177736, 493), (0.4546193122863773, 546), (0.255088520050049, 18), (0.030128574371337535, 890), (1.744834232330322, 859), (0.7680370330810549, 264), (0.5568385124206543, 681), (0.03634748458862269, 611), (0.14417276382446254, 656), (0.24343309402465785, 405), (0.35511350631713867, 252), (1.0532942771911618, 265), (0.18429145812988246, 273), (0.34944429397583043, 976), (0.06554613113403285, 800), (0.45769109725952184, 402), (1.4660325050354004, 959), (1.6884823799133297, 675), (0.5911412239074707, 659), (1.1442590713500973, 314), (1.046660995483398, 251), (0.1738630294799801, 974), (0.467600250244141, 32), (0.2567774772644045, 50), (0.26235599517822283, 21), (0.4506402015686035, 952), (0.2372648239135744, 777), (1.0729527473449707, 925), (0.7748534202575685, 78), (0.2535502433776857, 554), (1.836069869995117, 697), (1.8338622093200687, 499), (0.1416970252990719, 674), (0.45804300308227575, 90), (0.3535117149353031, 112), (0.24989442825317365, 837), (0.48454990386962926, 827), (0.352854537963867, 164), (0.6468287467956539, 327), (0.4366030693054199, 873), (0.06406545639038086, 209), (1.3525536537170408, 124), (1.0761656761169434, 98), (0.017946338653564098, 443), (0.3177923202514652, 917), (0.17627487182617152, 948), (0.5726838111877441, 280), (0.010076618194579723, 740), (0.5459609031677246, 446), (0.4631785392761234, 187), (0.038664436340331676, 673), (0.3496373176574705, 621), (1.0406571388244625, 789), (1.0495200157165527, 81), (0.7305109977722166, 331), (0.02322778701782191, 761), (0.7484194755554201, 840), (1.1405201911926266, 204), (1.2811791419982912, 216), (0.4826973915100101, 146), (0.5398446083068844, 435), (0.7482802391052248, 396), (1.1489315986633297, 587), (0.7538474082946776, 593), (0.7777192115783693, 370), (0.568427562713623, 575), (0.16071519851684535, 855), (1.3742602348327635, 239), (0.8396509170532225, 246), (0.7542670249938963, 993), (1.543266773223877, 119), (0.7589528083801271, 269), (0.6746245384216305, 79), (0.050374507904052734, 829), (0.0415806770324707, 968), (0.5389643669128414, 847), (0.2527558326721193, 194), (0.1233117103576662, 36), (0.8388417243957518, 126), (0.7835251808166506, 902), (0.33003797531127965, 726), (0.12730569839477557, 951), (1.0844650268554688, 842), (1.8673484802246092, 439), (0.341041851043701, 964), (0.7480446815490724, 633), (0.7900182723999025, 55), (0.8386444091796879, 766), (1.3818753242492674, 489), (0.06500015258789027, 256), (0.45677509307861364, 564), (0.35715036392211896, 832), (0.8242835044860843, 888), (0.453839206695557, 496), (0.34789171218872106, 457), (0.048272705078124645, 605), (2.4564080238342285, 871), (0.4607037544250492, 815), (0.3650063514709476, 321), (0.886456775665283, 363), (1.0464035034179684, 418), (0.36990385055541974, 223), (0.43605566024780273, 931), (0.7364571571350096, 926), (0.4937892913818356, 882), (0.758135509490967, 750), (0.07163333892822266, 992), (0.3560898780822752, 903), (0.3242581367492674, 202), (1.5599007606506348, 231), (0.7580163002014162, 652), (1.155458545684814, 154), (0.6653758049011227, 742), (0.6495023727416989, 60), (0.4557484626770023, 803), (1.1598158836364743, 978), (0.2533661842346193, 249), (0.2335904121398924, 255), (0.5494017601013184, 302), (0.03993711471557582, 155), (0.07957983016967773, 972), (0.25115804672241193, 937), (0.774462413787842, 190), (0.43267297744750977, 806), (0.04336652755737269, 43), (1.5574865341186523, 118), (0.4371762275695801, 369), (2.0423197746276855, 494), (0.6533247947692873, 460), (0.8438614845275882, 131), (0.23547773361206037, 37), (0.4434803962707523, 17), (0.07856512069702148, 936), (0.14480028152465785, 661), (0.4558234214782715, 27), (0.1585865974426266, 729), (0.6573201179504391, 24), (0.41873645782470703, 624), (0.5437308311462399, 664), (1.3433392524719237, 180), (0.6518799781799318, 357), (1.3774645805358885, 100), (0.48474874496459996, 433), (0.6682606697082516, 908), (1.4642304420471195, 16), (0.3347586631774906, 692), (0.36003332138061506, 947), (0.002587223052978871, 385), (1.1405545234680172, 650), (2.2257779121398924, 272), (0.7412736892700194, 364), (0.9443181991577152, 407), (0.7117088317871092, 454), (0.5289107322692868, 215), (0.15661439895629847, 895), (0.0410181045532223, 5), (0.31113834381103533, 87), (0.6427603721618649, 763), (0.13639945983886737, 586), (0.24313192367553693, 760), (1.162012672424316, 101), (0.8450310707092283, 442), (1.448279857635498, 725), (0.0038105010986324572, 213), (1.031514739990234, 775), (0.8369058609008793, 846), (0.14560756683349574, 731)]\n"
     ]
    }
   ],
   "source": [
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
